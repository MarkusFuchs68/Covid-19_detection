{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vHBZXj2FIzuT"},"outputs":[],"source":["# Libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Imports for model building\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import Dropout\n","\n","# Imports for image transformations\n","from tensorflow.keras.layers import RandomFlip\n","from tensorflow.keras.layers import RandomZoom\n","from tensorflow.keras.layers import RandomRotation\n","from tensorflow.keras.layers import RandomTranslation\n","\n","# Callbacks\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","\n","# Importing Keras utility for image dataset loading\n","from keras.utils import image_dataset_from_directory\n","\n","# Reporting\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from imblearn.metrics import classification_report_imbalanced\n","\n","# Imports for visualizations\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9hVbtv1IzuX"},"outputs":[],"source":["# Variables\n","dataset_folder = '../dataset' # We stored the masked files in this dataset folder\n","class_normal = 'normal'\n","class_lung_opacity = 'lung_opacity'\n","class_viral_pneumonia = 'viral_pneumonia'\n","class_covid = 'covid'"]},{"cell_type":"markdown","metadata":{"id":"i5vf4501IzuY"},"source":["### First try with Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"fV0BnjZsIzua"},"source":["##### Dataset loading and preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6r0F7wS7Izub"},"outputs":[],"source":["# For testing purposes, we try not loading all classes\n","normal_covid_classes = [class_normal, class_covid]\n","batch_size = 16 # Rather small for fine grained learning rate\n","\n","# First load the whole dataset with implicit resizing for VGG16\n","train_ds = image_dataset_from_directory(\n","    dataset_folder,\n","# don't give class names to the loader, so all classes are loaded\n","#    class_names = normal_covid_classes,\n","    validation_split=0.2,       # Fraction of the data used for validation\n","    subset=\"training\",          # Load the training data\n","    seed=42,                    # Seed for data splitting\n","    batch_size=batch_size,\n","    image_size=(224,224)        # Resize for compatibility with VGG16\n",")\n","\n","val_ds = image_dataset_from_directory(\n","    dataset_folder,\n","# don't give class names to the loader, so all classes are loaded\n","#    class_names = normal_covid_classes,\n","    validation_split=0.2,       # Fraction of the data used for validation\n","    subset=\"validation\",        # Load the validation data\n","    seed=42,\n","    batch_size=batch_size,\n","    image_size=(224,224)        # automatic resizing to the CNN input shape\n",")\n","\n","# Check, which class names found (should be our 4 (or less for trials)) and save it for later use\n","class_names = train_ds.class_names.copy()\n","print(class_names)\n","\n","# Adding optimization: caching and preloading\n","# This caused some errors, when only part of it is read, maybe switch it off\n","#train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","#val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKHw14wcIzuc"},"outputs":[],"source":["# Number of batches in the training dataset\n","print(\"Number of batch in train_ds:\", train_ds.cardinality().numpy())\n","\n","# Number of batches in the validation dataset\n","print(\"Number of batch in val_ds:\", val_ds.cardinality().numpy())\n","\n","# Let's check compatible input format for VGG16\n","for image, _ in train_ds.take(1):\n","    print(image.shape)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfOpEmRNIzuc"},"outputs":[],"source":["# We have imbalanced data, hence let's calculate class_weights to give it the model\n","from collections import Counter\n","\n","# Count occurrences of each class\n","class_counts = Counter()\n","total_count = 0\n","for images, labels in train_ds:\n","    for label in labels.numpy():  # Convert tensor to numpy\n","        class_counts[label] += 1\n","        total_count += 1\n","\n","# Convert label indices to class names\n","class_counts_named = {class_names[idx]: count for idx, count in class_counts.items()}\n","print(\"Class Counts:\", class_counts_named)\n","\n","class_weights_dict = { class_label: total_count / (len(class_counts) * class_counts[class_label])\n","                 for class_label, class_count in class_counts.items() }\n","print('Class Weights:', class_weights_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xYZ-husIzud"},"outputs":[],"source":["# Preprocessing for pretrained models in the same way\n","# the preprocessor comes with the pretrained model library\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y))\n","val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y))"]},{"cell_type":"markdown","metadata":{"id":"Gpf_D9oaIzud"},"source":["#### Try with VGG16 (imagenet pretrained)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sWgGJRjIzue"},"outputs":[],"source":["# Imports for using a pre-trained model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","# VGG16 model\n","base_model = VGG16(weights='imagenet', include_top=False)\n","\n","# Freeze the layers of VGG16\n","base_model.trainable = False\n","\n","# Model creation using the Functional API\n","inputs = Input(shape=(224, 224, 3))\n","\n","# Apply augmentations\n","#x = RandomRotation(0.1)(inputs)\n","#x = RandomTranslation(height_factor=0.1, width_factor=0.1)(x)\n","#x = RandomZoom(0.1)(x)\n","#x = RandomFlip(\"horizontal\")(x)\n","\n","# Build the model\n","x = base_model(inputs)\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","x = Dropout(rate=0.2)(x)\n","x = Dense(512, activation='relu')(x)\n","x = Dropout(rate=0.2)(x)\n","outputs = Dense(len(class_names), activation='softmax')(x)\n","\n","vgg16 = Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1-OMZ-XIzue"},"outputs":[],"source":["# Applied Callbacks\n","early_stopping = EarlyStopping(\n","                                patience=3, # Wait for 5 epochs before applying\n","                                min_delta=0.01, # If the loss function doesn't change by 1% after 3 epochs, either up or down, we stop\n","                                verbose=1, # Display the epoch at which training stops\n","                                mode='min',\n","                                monitor='val_loss')\n","\n","# A learning rate reduction callback to reduce the learning rate when the validation loss stagnates\n","reduce_learning_rate = ReduceLROnPlateau(\n","                                    monitor=\"val_loss\",\n","                                    patience=3, # If val_loss stagnates for 3 consecutive epochs based on the min_delta value\n","                                    min_delta=0.01,\n","                                    factor=0.1,  # Reduce the learning rate by a factor of 0.1\n","                                    cooldown=3,  # Wait 3 epochs before retrying\n","                                    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hugdi_k9Izue"},"outputs":[],"source":["# Compile with first idea of appropriate parameters\n","vgg16.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# Print how it looks like\n","vgg16.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-y7PGsMIzuf"},"outputs":[],"source":["# First training just on 10 batches (otherwise this would run endlessly on my machine)\n","history_model = vgg16.fit(train_ds,\n","                          epochs=10,\n","                          validation_data=val_ds,\n","                          class_weight=class_weights_dict,\n","                          callbacks=[early_stopping, reduce_learning_rate])\n","\n","# Save the model for potential later use\n","save_name = 'vgg16_' + str(len(class_names)) + '-classes.keras'\n","vgg16.save('vgg16.keras')"]},{"cell_type":"markdown","metadata":{"id":"2dSiCWUtIzuf"},"source":["##### Evalution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VpSYkwd3Izuf"},"outputs":[],"source":["def plot_learning_curve(history_model):\n","    plt.figure(figsize=(12,4))\n","\n","    plt.subplot(121)\n","    plt.plot(history_model.history['loss'])\n","    plt.plot(history_model.history['val_loss'])\n","    plt.title('Model loss by epoch')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='right')\n","\n","    plt.subplot(122)\n","    plt.plot(history_model.history['accuracy'])\n","    plt.plot(history_model.history['val_accuracy'])\n","    plt.title('Model accuracy by epoch')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='right')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2eidceFIzuf"},"outputs":[],"source":["plot_learning_curve(history_model)"]},{"cell_type":"markdown","metadata":{"id":"RXTnYWBFIzuf"},"source":["> We can see, that our model accuracy stagnates at appr. 70%\n","> It seems, that we cannot improve here much."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-97x0HLIzug"},"outputs":[],"source":["# Get true labels and predictions from the test dataset\n","def get_predictions_and_labels(model, dataset):\n","    true_labels = []\n","    pred_labels = []\n","\n","    for images, labels in dataset:\n","\n","        preds = model.predict(images, verbose=0)  # Get the model's predictions\n","        pred_labels.extend(np.argmax(preds, axis=-1))  # Get the predicted labels (argmax)\n","\n","        true_labels.extend(labels.numpy())  # Get the true labels\n","\n","    return np.array(true_labels), np.array(pred_labels)\n","\n","# Print a report with classification_report and heatmap confusion_matrix\n","def report_preds(y_true, y_pred):\n","    # Print the classification report (precision, recall, F1-score)\n","    print(classification_report_imbalanced(y_true, y_pred, target_names=class_names))\n","\n","    # Show also the non-normalized crosstab\n","    #display(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted']))\n","    ct = pd.crosstab(y_true, y_pred, rownames=['True'],colnames=['Predicted'])\n","    column_mapping = {index: class_name for index, class_name in enumerate(class_names)}\n","    ct = ct.rename(columns=column_mapping)\n","    ct.index = class_names\n","    display(ct)\n","\n","    # Display the confusion matrix\n","    plt.figure(figsize=(4, 4))  # Create a large figure for the confusion matrix\n","    cnf_matrix = confusion_matrix(y_true, y_pred, normalize='true')  # Compute the normalized confusion matrix\n","    sns.heatmap(cnf_matrix, cmap='Blues', annot=True, cbar=False, fmt=\".2f\")  # Plot the confusion matrix as a heatmap\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.xticks(ticks=np.arange(0.5,len(class_names)+0.5,1), labels=class_names, rotation=45, ha='right')\n","    plt.yticks(ticks=np.arange(0.5,len(class_names)+0.5,1), labels=class_names, rotation=45, ha='right')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUco2A3FIzug"},"outputs":[],"source":["y_true, y_pred = get_predictions_and_labels(vgg16, val_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrKJ4Th9Izug"},"outputs":[],"source":["report_preds(y_true, y_pred)\n","\n","# Conclusion: TBD"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}